{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4700 - Homework #2\n",
    "#### Due Date: Wednesday, 11/14 @ 6pm on CMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1: Introduction\n",
    "In this assignment, you will be implementing a decision tree learner in Python 3. Before we can start, let us import some functions that we will need. Please ensure the file *utils.py* is in the same directory as your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the decision tree learner will take in, as input, a dataset containing examples. These examples each contain a sequence of attributes. Given some target attribute, the learner will create a decision tree that will (hopefully) be shallow and, for each example in the dataset, output the correct value for the target attribute. Before we begin to code the learner, we will need to first define what exactly is a decision tree. First, let's begin by defining the leaf node of a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A leaf node of a decision tree which holds just a result.\n",
    "\"\"\"\n",
    "class DTLeaf:\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def __call__(self, example):\n",
    "        return self.result\n",
    "\n",
    "    def display(self, indent = 0):\n",
    "        print('Result =', self.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we define the internal (meaning non-leaf) node of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An internal node of a decision tree which tests an attribute, along with a\n",
    "dictionary of branches, one for each of the attribute's values.\n",
    "\"\"\"\n",
    "class DTInternal:\n",
    "    \"\"\"\n",
    "    Initialize by saying what attribute this node tests.\n",
    "    \"\"\"\n",
    "    def __init__(self, attr, attrName = None, defaultChild = None,\n",
    "                 branches = None):\n",
    "        self.attr = attr\n",
    "        self.attrName = attrName or attr\n",
    "        self.defaultChild = defaultChild\n",
    "        self.branches = branches or {}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Given an example, classify it using the attribute and the branches.\n",
    "    \"\"\"\n",
    "    def __call__(self, example):\n",
    "        attrVal = example[self.attr]\n",
    "        if attrVal in self.branches:\n",
    "            return self.branches[attrVal](example)\n",
    "        else:\n",
    "            return self.defaultChild(example)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Add a branch.\n",
    "    \"\"\"\n",
    "    def add(self, val, subtree):\n",
    "        self.branches[val] = subtree\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Prints out the decision tree starting at this internal node using recursion.\n",
    "    \"\"\"\n",
    "    def display(self, indent = 1):\n",
    "        print ('Testing attribute:', self.attrName)\n",
    "        for (val, subtree) in self.branches.items():\n",
    "            print(' ' * 4 * indent, self.attrName, '=', val, '==>', end = ' ')\n",
    "            subtree.display(indent + 1)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are 4 functions that will prove to be useful in your implementation of the decision tree learner. Take some time to go over what each function is doing and familiarize yourself with how to use them. This will make the assignment easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multiply each number by a constant such that the sum is 1.0\n",
    "\"\"\"\n",
    "def normalize(dist):\n",
    "    if isinstance(dist, dict):\n",
    "        total = sum(dist.values())\n",
    "        for key in dist:\n",
    "            dist[key] = dist[key] / total\n",
    "            assert 0 <= dist[key] <= 1, 'Probabilities must be between 0 and 1.'\n",
    "        return dist\n",
    "    total = sum(dist)\n",
    "    return [(n / total) for n in dist]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return a copy of the input with all occurrences of item removed.\n",
    "\"\"\"\n",
    "def removeAll(item, seq):\n",
    "    if isinstance(seq, str):\n",
    "        return seq.replace(item, '')\n",
    "    else:\n",
    "        return [x for x in seq if x != item]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Randomly shuffle a copy of the input.\n",
    "\"\"\"\n",
    "def shuffled(seq):\n",
    "    items = list(seq)\n",
    "    random.shuffle(items)\n",
    "    return items\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return an element with the highest value according to key, which is a function.\n",
    "We break ties by first shuffling the input.\n",
    "\"\"\"\n",
    "def argmaxRandomTie(seq, key):\n",
    "    return max(shuffled(seq), key = key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a decision tree now defined, we can begin to code the learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2: The First Half of the Learner\n",
    "We will be modeling our code closely to the pseudo-code from the textbook. Your first task is to implement the function *__pluralityVal__*. Recall that, given examples, we define the plurality value as the value of the target attribute that appears most often. As a hint, you will likely need to use both *__count__* (defined below) as well as *__argmaxRandomTie__*. Note, to make *__pluralityVal__* work easier with the learner, it should return a leaf node containing the plurality value, rather than just the value itself. We have also provided a small test to check if your function is returning the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count the number of examples that have example[attr] = val.\n",
    "\"\"\"\n",
    "def count(attr, val, examples):\n",
    "    return sum(e[attr] == val for e in examples)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def pluralityVal(examples, values, target):\n",
         "frequencyStorage = []"
         "for singleVal in values[target]:\n"
         "frequencyStorage.append([singleVal,count(target,singleVal,examples)])\n"
         "leaf = argmaxRandomTie(frequencyStorage, key = lambda x: x[1])\n"
         "print(leaf.result)\n"

    "\n",
    "\n",
    "# Test for pluralityVal\n",
    "te = [[3,2,1], [1,0,1], [4,3,2], [3,0,0], [5,2,1]]\n",
    "tv = [[1,2,3,4,5], [0,1,2,3], [0,1,2]]\n",
    "tt = 2\n",
    "assert pluralityVal(te, tv, tt).result == 1, 'Failed pluralityVal test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to determine if all the examples have the same classification. We will do this in the function *__allSameClass__*, which should simply return a Boolean value. Once again, a couple of tests have been added to ensure you are returning the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def allSameClass(examples, target):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Tests for allSameClass\n",
    "te = [[3,2,1], [1,0,1], [4,3,2], [3,0,0], [5,2,1]]\n",
    "tt = 1\n",
    "assert not allSameClass(te, tt), 'Failed allSameClass test #1'\n",
    "te = [[3,0,1], [1,0,1], [4,0,2], [3,0,0], [5,0,1]]\n",
    "assert allSameClass(te, tt), 'Failed allSameClass test #2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have completed enough functions for the first half of the decision tree learner. We will now focus on the second half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 3: The Second Half of the Learner\n",
    "We need to some way to determine the most important attribute in our dataset that we should split next on for our tree. There are many ways for us to define \"importance.\" One common way to do so is by looking at the information gain that an attribute provides. This gain is defined in terms of the entropy. Recall that entropy can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(V) &= - \\sum_{k} P(v_k)\\ log_2 P(v_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the above definition relies on probabilities. Can you assume that the attribute values being passed in are probabilities? Consider the answer to this in your implementation of this equation in the function *__entropy__*. If correct, you should be able to pass the 2 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def entropy(values):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Tests for entropy\n",
    "tv = [13, 5, 2, 20, 4, 10, 4]\n",
    "assert abs(entropy(tv) - 2.4549947941466774) <= 1e-3, 'Failed entropy test #1'\n",
    "tv = [0, 0, 5, 0, 0, 0, 0]\n",
    "assert entropy(tv) == 0.0, 'Failed entropy test #2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the entropy now being calculated, we are able to determine the information gain of an attribute. The calculation for this is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "B(q) &= -[q\\ log_2 q + (1 - q)\\ log_2 (1 - q)] \\\\\n",
    "Remainder(A) &= \\sum_{k = 1}^{d} \\frac{p_k + n_k}{p + n}\\ B(\\frac{p_k}{p_k + n_k}) \\\\\n",
    "InfoGain(A) &= B(\\frac{p}{p + n}) - Remainder(A)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As a reminder, \\\\(q\\\\) is the probability of a Boolean random variable being true, \\\\(p\\\\) is the number of positive examples in the training set, \\\\(n\\\\) is the number of negative examples, while their subscripted versions are analogous but for the different values of attribute \\\\(A\\\\). You will need to implement the above calculations in the function *__infoGain__*. You might find it useful to use *__splitBy__* (defined below), since calculating \\\\(Remainder\\\\) requires us to split an attribute \\\\(A\\\\) by its \\\\(d\\\\) distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return a list of (value, examples) pairs for each val of attr.\n",
    "\"\"\"\n",
    "def splitBy(attr, examples, values):\n",
    "    return [(v, [e for e in examples if e[attr] == v]) for v in values[attr]]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def infoGain(attr, examples, values, target):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Tests for infoGain\n",
    "te = [[3,2,1], [1,0,1], [4,3,2], [3,0,0], [5,2,1]]\n",
    "tv = [[1,2,3,4,5], [0,1,2,3], [0,1,2]]\n",
    "tt = 2\n",
    "ta = 0\n",
    "assert abs(infoGain(ta, te, tv, tt) - 0.9709505944546687) <= 1e-3, \\\n",
    "       'Failed infoGain test #1'\n",
    "tt = 1\n",
    "assert abs(infoGain(ta, te, tv, tt) - 1.121928094887362) <= 1e-3, \\\n",
    "       'Failed infoGain test #2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last bit of functionality that our learner needs is to determine which attribute we should choose to split on. To recap, the decision tree learner greedily chooses the attribute that provides the most information gain as the splitting point. Capture this logic in the function *__chooseAttr__*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def chooseAttr(attrs, examples, values, target):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Test for chooseAttr\n",
    "te = [[6,7,8,7,0], [7,3,4,0,0], [1,0,2,3,3], [8,5,9,1,1], [9,1,5,4,2],\n",
    "      [9,5,6,8,3], [4,6,0,1,6], [8,5,4,4,2], [0,3,7,2,6], [7,9,3,1,2]]\n",
    "tv = [[0,1,2,3,4,5,6,7,8,9] for i in range(10)]\n",
    "tt = 3\n",
    "ta = [0,1,2,4]\n",
    "assert chooseAttr(ta, te, tv, tt) == 2, 'Failed chooseAttr test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the necessary functions to implement our learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 4: Putting It All Together\n",
    "With everything now in place, we should now be able to implement the decision tree learner in full. For easier reference, the pseudo-code from the textbook (fig. 18.5) has been reproduced below. Use this to help guide your implementation in the function *__DTLearner__*.\n",
    "\n",
    "**function** DECISION-TREE-LEARNING(*examples*, *attributes*, *parent_examples*) **returns** a tree<br/>\n",
    "&nbsp;&nbsp;**if** *examples* is empty **then return** PLURALITY-VALUE(*parent_examples*)<br/>\n",
    "&nbsp;&nbsp;**else if** all *examples* have the same classification **then return** the classification<br/>\n",
    "&nbsp;&nbsp;**else if** *attributes* is empty **then return** PLURALITY-VALUE(*examples*)<br/>\n",
    "&nbsp;&nbsp;**else**<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\\\\(A \\leftarrow argmax_{a \\in attributes}\\\\) IMPORTANCE(\\\\(a\\\\), *examples*)<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;*tree* \\\\(\\leftarrow\\\\) a new decision tree with root test \\\\(A\\\\)<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for each** value \\\\(v_k\\\\) of \\\\(A\\\\) **do**<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*exs* \\\\(\\leftarrow\\\\) {\\\\(e : e \\in\\\\) *examples* **and** *e.A* = \\\\(v_k\\\\)}<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*subtree* \\\\(\\leftarrow\\\\) DECISION-TREE-LEARNING(*exs*, *attributes* âˆ’ \\\\(A\\\\), *examples*)<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add a branch to *tree* with label (\\\\(A\\\\) = \\\\(v_k\\\\)) and subtree *subtree*<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**return** *tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def DTLearner(examples, attrs, values, target, attrNames, parentExamples = ()):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test for DTLearner\n",
    "\n",
    "Output should match:\n",
    "\n",
    "Testing attribute: 7\n",
    "     7 = 0 ==> Result = 1\n",
    "     7 = 1 ==> Result = 0\n",
    "     7 = 2 ==> Testing attribute: 11\n",
    "         11 = 0 ==> Result = 0\n",
    "         11 = 1 ==> Result = 2\n",
    "         11 = 2 ==> Result = 1\n",
    "\"\"\"\n",
    "te = [[1,1,1,1,1,1,2,2,2,0,2,0,0,1,2,0,0,2,2,0],\n",
    "      [2,0,1,2,1,1,1,2,2,2,2,1,0,2,2,0,2,2,2,2],\n",
    "      [1,2,1,1,1,2,2,2,0,1,2,2,2,2,2,1,2,0,2,1],\n",
    "      [2,2,2,1,0,1,2,2,2,0,2,1,0,1,1,1,0,2,0,2],\n",
    "      [0,1,1,0,0,0,2,0,1,0,1,2,2,2,2,0,0,0,1,1],\n",
    "      [0,2,0,0,0,1,0,1,0,2,1,1,2,0,2,2,0,2,0,0],\n",
    "      [0,0,1,2,0,0,0,0,1,2,0,0,2,0,0,0,0,2,1,1],\n",
    "      [0,2,2,1,1,0,0,2,2,0,2,1,1,0,0,2,0,2,1,2],\n",
    "      [1,0,2,0,1,2,2,1,0,1,0,2,1,2,0,0,1,1,2,0],\n",
    "      [0,2,0,1,2,1,1,1,1,0,1,2,2,0,1,2,1,0,0,0]]\n",
    "tv = [[j for j in range(3)] for i in range(20)]\n",
    "tt = 19\n",
    "ta = [i for i in range(19)]\n",
    "tn = ta\n",
    "DTLearner(te, ta, tv, tt, tn).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the above is working, we should now be able to train, display, and test our decision tree! The code block below will load in the restaurant data from the textbook, use it as training data for our learner, output the learned decision tree, and then use that to try to classify a test point. To help with this process, we use the *__DTLCaller__* to extract the relevant information from our dataset and pass that into the learner. Please ensure you have the *restaurant.csv* in the same directory as your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Front-end caller for the decision tree learner. Given an input dataset, it will\n",
    "extract the necessary information and pass it in to the learner.\n",
    "\"\"\"\n",
    "def DTLCaller(dataset):\n",
    "    return DTLearner(dataset.examples, dataset.inputs, dataset.values,\n",
    "                     dataset.target, dataset.attrNames)\n",
    "\n",
    "\n",
    "# Load the restaurant dataset from a CSV file\n",
    "restaurant = Dataset(name = 'restaurant', target = 'Wait',\n",
    "                     attrNames = 'Alternate Bar Fri/Sat Hungry Patrons Price ' +\n",
    "                     'Raining Reservation Type WaitEstimate Wait')\n",
    "\n",
    "\n",
    "# Feed the dataset into the decision tree learner\n",
    "dtlRestaurant = DTLCaller(restaurant)\n",
    "\n",
    "\n",
    "# Display the decision tree that is learned\n",
    "dtlRestaurant.display()\n",
    "\n",
    "\n",
    "# Try classifying new test data\n",
    "print(dtlRestaurant(['Yes','No','No','Yes','Full',\n",
    "                     '$$','No','No','Italian','0-10']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use additional datasets. For example, the code block below uses a zoo dataset to train our learner and find a decision tree for our data. The zoo dataset contains a list of animals, each of which has different features about the animal, along with what kind of animal it is. We use our learned decision tree to determine what kind of animal a dragonfly is. Please ensure you have the *zoo.csv* in the same directory as your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the zoo dataset from a CSV file\n",
    "zoo = Dataset(name = 'zoo', target = 'type', exclude = ['name'],\n",
    "              attrNames = 'name hair feathers eggs milk airborne aquatic ' +\n",
    "              'predator toothed backbone breathes venomous fins legs tail ' +\n",
    "              'domestic catsize type')\n",
    "\n",
    "\n",
    "# Feed the zoo dataset into the decision tree learner\n",
    "dtlZoo = DTLCaller(zoo)\n",
    "\n",
    "\n",
    "# Display the decision tree that is learned\n",
    "dtlZoo.display()\n",
    "\n",
    "\n",
    "# Try classifying new test data\n",
    "print(dtlZoo(['dragonfly',0,0,1,0,1,0,1,0,0,1,0,0,6,0,0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 5: Submission\n",
    "You will only be submitting your Jupyter Notebook file, *hwk2.ipynb*. Do not worry about submitting the additional files (*utils.py*, *restaurant.csv*, *zoo.csv*). Furthermore, as a reminder, part of your grade is your documentation. Each of the functions that you implemented as part of this assignment **must** be documented. Explain what the function is doing, its inputs/outputs, the logic behind your implementation, fancy Python ~~hacks~~ tricks, etc. Failure to include detailed documentation will result in a penalty.\n",
    "\n",
    "Please upload your *hwk2.ipynb* file to CMS by **Wednesday, 11/14 @ 6pm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
